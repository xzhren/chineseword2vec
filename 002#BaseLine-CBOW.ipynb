{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import random\n",
    "from tempfile import gettempdir\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pickle\n",
    "\n",
    "from tensorflow.contrib.tensorboard.plugins import projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zh_wiki_id = open(\"data/zh_wiki_id_w_d\").readline()\n",
    "word_to_id = pickle.load(open(\"data/word_to_id_w_d.pkl\", \"rb\"))\n",
    "id_to_word = pickle.load(open(\"data/id_to_word_w_d.pkl\", \"rb\"))\n",
    "\n",
    "# zh_wiki_id = open(\"data/zh_wiki_id\").readline()\n",
    "# word_to_id = pickle.load(open(\"data/word_to_id.pkl\", \"rb\"))\n",
    "# id_to_word = pickle.load(open(\"data/id_to_word.pkl\", \"rb\"))\n",
    "# word_count = pickle.load(open(\"data/count.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(507260, 507260, 862838467)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (507260, 507260, 862838467)\n",
    "len(id_to_word), len(word_to_id), len(zh_wiki_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getWord(data, num, data_index):\n",
    "    sub_data_string = data[data_index:data_index+num*(6+1)]\n",
    "    result = []\n",
    "    for index, item in enumerate(sub_data_string.split()):\n",
    "        if index == num: break\n",
    "        data_index += len(item) + 1\n",
    "        result.append(int(item))\n",
    "    if len(result) < num:\n",
    "        return getWord(data, num, 0)\n",
    "    assert len(result) == num\n",
    "    return result, data_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_batch(batch_size, skip_window, num_skips):\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size, num_skips), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1  # [ skip_window target skip_window ]\n",
    "    assert batch_size >= span\n",
    "    buffer = collections.deque(maxlen=span)  # pylint: disable=redefined-builti\n",
    "    \n",
    "    result, data_index = getWord(zh_wiki_id, span, data_index)\n",
    "    buffer.extend(result)\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        context_words = [w for w in range(span) if w != skip_window]\n",
    "        batch[i, :] = [buffer[token] for idx, token in enumerate(context_words)]\n",
    "        labels[i, 0] = buffer[skip_window]\n",
    "        result, data_index = getWord(zh_wiki_id, 1, data_index)\n",
    "        buffer.append(result[0])\n",
    "        if data_index > len(zh_wiki_id):\n",
    "            result, data_index = getWord(zh_wiki_id, span-1, 0)\n",
    "            buffer.extend(result)\n",
    "        if i == batch_size - span:\n",
    "            last_index = data_index\n",
    "            \n",
    "    # Backtrack a little bit to avoid skipping words in the end of a batch\n",
    "    data_index = last_index\n",
    "    return batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1348 数学 501 利用 -> 9 是\n",
      "9 是 237319 符号语言 -> 501 利用\n",
      "501 利用 141 研究 -> 237319 符号语言\n",
      "237319 符号语言 894 数量 -> 141 研究\n",
      "141 研究 5 、 -> 894 数量\n",
      "894 数量 499 结构 -> 5 、\n",
      "5 、 5 、 -> 499 结构\n",
      "499 结构 331 变化 -> 5 、\n"
     ]
    }
   ],
   "source": [
    "data_index = 0\n",
    "\n",
    "batch, labels = generate_batch(batch_size=8, skip_window=1, num_skips=2*1)\n",
    "for i in range(8):\n",
    "    print(batch[i, 0], id_to_word[batch[i, 0]],\n",
    "          batch[i, 1], id_to_word[batch[i, 1]],\n",
    "          '->', labels[i, 0], id_to_word[labels[i, 0]])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1296 数学 462 利用 -> 4 是\n",
    "4 是 235572 符号语言 -> 462 利用\n",
    "462 利用 116 研究 -> 235572 符号语言\n",
    "235572 符号语言 850 数量 -> 116 研究\n",
    "116 研究 460 结构 -> 850 数量\n",
    "850 数量 301 变化 -> 460 结构\n",
    "460 结构 53 以及 -> 301 变化\n",
    "301 变化 611 空间 -> 53 以及"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Build & Train Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"embedding_lookup:0\", shape=(11, 200), dtype=float32) Tensor(\"truediv:0\", shape=(507260, 200), dtype=float32) Tensor(\"MatMul:0\", shape=(11, 507260), dtype=float32)\n",
      "---- top k same word\n",
      "test_embeddings Tensor(\"embedding_lookup_1:0\", dtype=float32)\n",
      "normalized_embeddings Tensor(\"truediv:0\", shape=(507260, 200), dtype=float32)\n",
      "similarity_test: Tensor(\"MatMul_1:0\", shape=(?, 507260), dtype=float32)\n",
      "similarity_test_top_k_value: Tensor(\"TopKV2:0\", shape=(?, 5), dtype=float32)\n",
      "similarity_test_top_k_index: Tensor(\"TopKV2:1\", shape=(?, 5), dtype=int32)\n",
      "---- analogical reasoning\n",
      "test_embeddings: Tensor(\"embedding_lookup_3:0\", dtype=float32, device=/device:GPU:0)\n",
      "test_embs: Tensor(\"split_1:0\", dtype=float32) Tensor(\"split_1:1\", dtype=float32)\n",
      "test_embs: Tensor(\"split_1:2\", dtype=float32) Tensor(\"split_1:3\", dtype=float32)\n",
      "test_result Tensor(\"Add:0\", dtype=float32)\n",
      "normalized_embeddings Tensor(\"truediv:0\", shape=(507260, 200), dtype=float32)\n",
      "analogical_similarity Tensor(\"Squeeze:0\", dtype=float32)\n",
      "analogical_top_k_value: Tensor(\"TopKV2_1:0\", dtype=float32)\n",
      "analogical_top_k_index: Tensor(\"TopKV2_1:1\", dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 200\n",
    "batch_size = 100 # 0509 change\n",
    "# batch_size = 256\n",
    "embedding_size = 200    # Dimension of the embedding vector.\n",
    "skip_window = 10    # How many words to consider left and right.\n",
    "num_skips = 2*skip_window    # How many times to reuse an input to generate a label.\n",
    "num_sampled = 100    # Number of negative examples to sample.\n",
    "# num_sampled = 128    # Number of negative examples to sample.\n",
    "vocabulary_size = len(id_to_word)\n",
    "\n",
    "# We pick a random validation set to sample nearest neighbors. Here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent. These 3 variables are used only for\n",
    "# displaying model accuracy, they don't affect calculation.\n",
    "# valid_size = 16    # Random set of words to evaluate similarity on.\n",
    "# valid_window = 100    # Only pick dev samples in the head of the distribution.\n",
    "# valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "# valid_examples = list(range(1, 10))\n",
    "valid_examples = list(range(280, 291))\n",
    "valid_size = len(valid_examples)\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    with tf.name_scope('inputs'):\n",
    "        train_inputs = tf.placeholder(tf.int32, shape=[batch_size, num_skips])\n",
    "        train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "        valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "        ### rxz\n",
    "        test_dataset = tf.placeholder(tf.int32, shape=None)\n",
    "\n",
    "    # Ops and variables pinned to the CPU because of missing GPU implementation\n",
    "    with tf.device('/gpu:0'):\n",
    "    # Look up embeddings for inputs.\n",
    "        with tf.name_scope('embeddings'):\n",
    "            embeddings = tf.Variable(\n",
    "                    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "            embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "            # take mean of embeddings of context words for context embedding\n",
    "            embed_context = tf.reduce_mean(embed, 1)\n",
    "\n",
    "    with tf.device('/gpu:0'):\n",
    "        # Construct the variables for the NCE loss\n",
    "        with tf.name_scope('weights'):\n",
    "            nce_weights = tf.Variable(\n",
    "                    tf.truncated_normal(\n",
    "                            [vocabulary_size, embedding_size],\n",
    "                            stddev=1.0 / math.sqrt(embedding_size)))\n",
    "        with tf.name_scope('biases'):\n",
    "            nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # Compute the average NCE loss for the batch.\n",
    "    # tf.nce_loss automatically draws a new sample of the negative labels each\n",
    "    # time we evaluate the loss.\n",
    "    # Explanation of the meaning of NCE loss:\n",
    "    #     http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\n",
    "    \n",
    "#     with tf.device('/gpu:2'):\n",
    "    with tf.name_scope('loss'):\n",
    "#         loss = tf.reduce_mean(\n",
    "#             tf.nn.nce_loss(nce_weights, nce_biases, embed_context, train_labels,\n",
    "#                            num_sampled, vocabulary_size))\n",
    "#         print(train_labels, embed_context)\n",
    "        loss = tf.reduce_mean(\n",
    "                tf.nn.nce_loss(\n",
    "                        weights=nce_weights,\n",
    "                        biases=nce_biases,\n",
    "                        labels=train_labels,\n",
    "                        inputs=embed_context,\n",
    "#                         labels=embed_context,\n",
    "#                         inputs=train_labels,\n",
    "                        num_sampled=num_sampled,\n",
    "                        num_classes=vocabulary_size))\n",
    "\n",
    "    # Add the loss value as a scalar to summary.\n",
    "    tf.summary.scalar('loss', loss)\n",
    "\n",
    "    # Construct the SGD optimizer using a learning rate of 1.0.\n",
    "    with tf.name_scope('optimizer'):\n",
    "#         optimizer = tf.train.GradientDescentOptimizer(0.01).minimize(loss)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(1).minimize(loss)\n",
    "#         optimizer = tf.train.AdamOptimizer(0.01).minimize(loss)\n",
    "\n",
    "    # Compute the cosine similarity between minibatch examples and all embeddings.\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    \n",
    "    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "    similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "    print(valid_embeddings, normalized_embeddings, similarity)\n",
    "    ### top k same word\n",
    "    print(\"---- top k same word\")\n",
    "    test_embeddings = tf.nn.embedding_lookup(normalized_embeddings, test_dataset)\n",
    "    print(\"test_embeddings\", test_embeddings)\n",
    "    print(\"normalized_embeddings\", normalized_embeddings)\n",
    "    similarity_test = tf.matmul(test_embeddings, normalized_embeddings, transpose_b=True)\n",
    "    print(\"similarity_test:\", similarity_test)\n",
    "    similarity_test_top_k_value, similarity_test_top_k_index  = tf.nn.top_k(similarity_test, k=5)\n",
    "    print(\"similarity_test_top_k_value:\", similarity_test_top_k_value)\n",
    "    print(\"similarity_test_top_k_index:\", similarity_test_top_k_index)\n",
    "    ### word similarity\n",
    "    test_embeddings = tf.nn.embedding_lookup(normalized_embeddings, test_dataset)\n",
    "    test_emb1, test_emb2 = tf.split(test_embeddings, [1, 1], 0)\n",
    "    similarity_smi_test = tf.matmul(test_emb1, test_emb2, transpose_b=True)\n",
    "    ### analogical reasoning\n",
    "    print(\"---- analogical reasoning\")\n",
    "    test_embeddings = tf.nn.embedding_lookup(embeddings, test_dataset)\n",
    "    print(\"test_embeddings:\", test_embeddings)\n",
    "    test_emb1, test_emb2, test_emb3, test_emb4 = tf.split(test_embeddings, [1, 1, 1, 1], 0)\n",
    "    print(\"test_embs:\", test_emb1, test_emb2)\n",
    "    print(\"test_embs:\", test_emb3, test_emb4)\n",
    "#     test_result_tmp = tf.subtract(test_emb1, test_emb2)\n",
    "    test_result = tf.add(tf.subtract(test_emb1, test_emb2), test_emb3)\n",
    "#     assert_sum_1_shape = tf.shape(test_result)\n",
    "#     assert_sum_1 = tf.reduce_sum(test_result, axis=-1)\n",
    "    analogical_product = tf.matmul(test_result, test_emb4, transpose_b=True)\n",
    "    analogical_norm_1 = tf.sqrt(tf.reduce_sum(tf.square(test_result), axis=0))\n",
    "    analogical_norm_2 = tf.sqrt(tf.reduce_sum(tf.square(test_emb4), axis=0))\n",
    "    analogical_smi_test = analogical_product / (analogical_norm_1 * analogical_norm_2)\n",
    "    print(\"test_result\", test_result)\n",
    "    print(\"normalized_embeddings\", normalized_embeddings)\n",
    "#     analogical_similarity = tf.squeeze(tf.matmul(tf.reshape(test_result, [1,-1]), normalized_embeddings, transpose_b=True))\n",
    "    test_result_norm = test_result / analogical_norm_1\n",
    "    analogical_similarity = tf.squeeze(tf.matmul(test_result_norm, normalized_embeddings, transpose_b=True))\n",
    "    print(\"analogical_similarity\", analogical_similarity)\n",
    "    analogical_top_k_value, analogical_top_k_index  = tf.nn.top_k(tf.squeeze(analogical_similarity), k=5)\n",
    "    print(\"analogical_top_k_value:\", analogical_top_k_value)\n",
    "    print(\"analogical_top_k_index:\", analogical_top_k_index)\n",
    "\n",
    "    # Merge all summaries.\n",
    "    merged = tf.summary.merge_all()\n",
    "\n",
    "    # Add variable initializer.\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Create a saver.\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_steps = 20000001\n",
    "# log_dir = \"./log_002_baseline_cbow/\"\n",
    "log_dir = \"./log_002_best_baseline/\"\n",
    "\n",
    "tfconfig = tf.ConfigProto()\n",
    "tfconfig.gpu_options.allow_growth = True\n",
    "\n",
    "with tf.Session(graph=graph, config=tfconfig) as session:\n",
    "    # Open a writer to write summaries.\n",
    "    writer = tf.summary.FileWriter(log_dir, session.graph)\n",
    "\n",
    "    # We must initialize all variables before we use them.\n",
    "#     init.run()\n",
    "#     saver = tf.train.import_meta_graph('./checkpoint_dir/MyModel-1000.meta')\n",
    "    saver.restore(session, tf.train.latest_checkpoint(log_dir))\n",
    "    print('Initialized')\n",
    "\n",
    "    average_loss = 0\n",
    "    start_index = 14640000\n",
    "    for step in xrange(start_index, num_steps):\n",
    "        batch_inputs, batch_labels = generate_batch(batch_size, skip_window=skip_window, num_skips=num_skips)\n",
    "        feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "\n",
    "        # Define metadata variable.\n",
    "        run_metadata = tf.RunMetadata()\n",
    "\n",
    "        # We perform one update step by evaluating the optimizer op (including it\n",
    "        # in the list of returned values for session.run()\n",
    "        # Also, evaluate the merged op to get all summaries from the returned \"summary\" variable.\n",
    "        # Feed metadata variable to session for visualizing the graph in TensorBoard.\n",
    "        _, summary, loss_val = session.run(\n",
    "                [optimizer, merged, loss],\n",
    "                feed_dict=feed_dict,\n",
    "                run_metadata=run_metadata)\n",
    "        average_loss += loss_val\n",
    "        \n",
    "        # Add returned summaries to writer in each step.\n",
    "        writer.add_summary(summary, step)\n",
    "        # Add metadata to visualize the graph for the last run.\n",
    "        if step == (num_steps - 1):\n",
    "            writer.add_run_metadata(run_metadata, 'step%d' % step)\n",
    "\n",
    "        if step % 2000 == 0:\n",
    "            if step > 0 and step != start_index:\n",
    "                average_loss /= 2000\n",
    "            # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "            print('Average loss at step ', step, ': ', average_loss, \":\", data_index)\n",
    "            average_loss = 0\n",
    "\n",
    "        # Note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "        if step % 10000 == 0:\n",
    "            sim = similarity.eval()\n",
    "            for i in xrange(valid_size):\n",
    "                valid_word = id_to_word[valid_examples[i]]\n",
    "                top_k = 8    # number of nearest neighbors\n",
    "                nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "                log_str = 'Nearest to %s:' % valid_word\n",
    "                for k in xrange(top_k):\n",
    "                    close_word = id_to_word[nearest[k]]\n",
    "                    log_str = '%s %s,' % (log_str, close_word)\n",
    "                print(log_str)\n",
    "                \n",
    "            # Save the model for checkpoints.\n",
    "            saver.save(session, os.path.join(log_dir, 'model.ckpt'), global_step=step)\n",
    "        \n",
    "        if step % 1000000 == 0 and step != start_index:\n",
    "            word2vec = embeddings.eval()\n",
    "            print(word2vec.shape, type(word2vec))\n",
    "            np.save(\"result/002#word_embedding_best_\"+str(step), word2vec)\n",
    "            \n",
    "    final_embeddings = normalized_embeddings.eval()\n",
    "\n",
    "    # Write corresponding labels for the embeddings.\n",
    "    with open(log_dir + '/metadata.tsv', 'w') as f:\n",
    "        for i in xrange(vocabulary_size):\n",
    "            f.write(id_to_word[i] + '\\n')\n",
    "\n",
    "    # Save the model for checkpoints.\n",
    "    saver.save(session, os.path.join(log_dir, 'model.ckpt'), global_step=step)\n",
    "\n",
    "    # Create a configuration for visualizing embeddings with the labels in TensorBoard.\n",
    "    config = projector.ProjectorConfig()\n",
    "    embedding_conf = config.embeddings.add()\n",
    "    embedding_conf.tensor_name = embeddings.name\n",
    "    embedding_conf.metadata_path = os.path.join(log_dir, 'metadata.tsv')\n",
    "    projector.visualize_embeddings(writer, config)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Initialized\n",
    "Average loss at step  14640000 :  4.0561466217 : 420\n",
    "Nearest to 音乐: 流行音乐, 唱片, 音乐创作, 古典音乐, 作曲家, 音乐家, 乐曲, 歌唱,\n",
    "Nearest to 地方: 地区, 省份, 城市, 区域, 各地, 地点, 巡查, 县内,\n",
    "Nearest to 社会: 知识分子, 道德, 价值观, 社会学, 人际关系, 社会秩序, 心理, 文学,\n",
    "Nearest to 服务: 运营, 业务, 营运, 邮件, 长途, 运作, 营办, 功能,\n",
    "Nearest to 均: 皆, 都, 均会, 分别, 仍, 中均, 一律, 亦,\n",
    "Nearest to 型: W型, 千赫, 形, 辆, 线, 分及, 倍径, 式,\n",
    "Nearest to 学生: 同学, 师生, 大学生, 留学生, 考生, 毕业生, 老师, 入学,\n",
    "Nearest to 今: 今属, 治今, 今日, 现, 今天, 现今, 棋王, 泉源,\n",
    "Nearest to 受到: 受, 深受, 遭受, 备受, 不受, 遭到, 广受, 饱受,\n",
    "Nearest to 事件: 惨案, 事故, 事情, 爆炸案, 案件, 恐怖袭击, 空难, 事变,\n",
    "Nearest to 经济: 国民经济, 经济学, 金融, 农业, 旅游业, 工商业, 外汇, 经济学家,\n",
    "Average loss at step  14642000 :  3.81943944275 : 780447\n",
    "Average loss at step  14644000 :  3.68332403046 : 1562474\n",
    "Average loss at step  14646000 :  3.69087736523 : 2342531\n",
    "Average loss at step  14648000 :  3.78545127767 : 3145912\n",
    "Average loss at step  14650000 :  3.82116334927 : 3938010\n",
    "Nearest to 音乐: 流行音乐, 唱片, 古典音乐, 作曲家, 音乐创作, 音乐家, 乐曲, 歌唱,\n",
    "Nearest to 地方: 地区, 省份, 区域, 城市, 巡查, 各地, 县内, 地点,\n",
    "Nearest to 社会: 知识分子, 道德, 价值观, 社会学, 社会秩序, 心理, 贫困, 社会主义,\n",
    "Nearest to 服务: 运营, 业务, 营运, 邮件, 长途, 营办, 运作, 邮政局,\n",
    "Nearest to 均: 皆, 都, 均会, 分别, 仍, 中均, 一律, 亦,\n",
    "Nearest to 型: W型, 千赫, 形, 辆, 线, _, 式, 倍径,\n",
    "Nearest to 学生: 同学, 师生, 大学生, 留学生, 考生, 毕业生, 老师, 入学,\n",
    "Nearest to 今: 今属, 治今, 今日, 现, 今天, 现今, 棋王, 泉源,\n",
    "Nearest to 受到: 受, 深受, 遭受, 备受, 不受, 遭到, 饱受, 广受,\n",
    "Nearest to 事件: 惨案, 事故, 事情, 爆炸案, 案件, 空难, 恐怖袭击, 事变,\n",
    "Nearest to 经济: 国民经济, 经济学, 金融, 农业, 外汇, 旅游业, 工商业, 失业率,\n",
    "Average loss at step  14652000 :  3.75037097764 : 4719137\n",
    "Average loss at step  14654000 :  3.84822692025 : 5503399\n",
    "Average loss at step  14656000 :  3.86127741909 : 6292282\n",
    "Average loss at step  14658000 :  3.80316126156 : 7071571\n",
    "Average loss at step  14660000 :  3.82215192509 : 7851000\n",
    "Nearest to 音乐: 流行音乐, 作曲家, 古典音乐, 唱片, 音乐创作, 音乐家, 乐曲, 歌唱,\n",
    "Nearest to 地方: 地区, 省份, 区域, 城市, 巡查, 各地, 县内, 首长,\n",
    "Nearest to 社会: 知识分子, 价值观, 道德, 社会学, 社会秩序, 心理, 贫困, 社会主义,\n",
    "Nearest to 服务: 运营, 业务, 营运, 邮件, 长途, 营办, 邮政局, 工作,\n",
    "Nearest to 均: 皆, 都, 均会, 分别, 仍, 中均, 亦, 一律,\n",
    "Nearest to 型: W型, 千赫, 形, 辆, -, 式, 线, 倍径,\n",
    "Nearest to 学生: 同学, 师生, 大学生, 留学生, 考生, 毕业生, 老师, 班级,\n",
    "Nearest to 今: 今属, 治今, 今日, 现, 今天, 现今, 棋王, 泉源,\n",
    "Nearest to 受到: 受, 深受, 遭受, 备受, 不受, 遭到, 饱受, 广受,\n",
    "Nearest to 事件: 惨案, 事故, 事情, 爆炸案, 案件, 空难, 恐怖袭击, 事变,\n",
    "Nearest to 经济: 国民经济, 经济学, 金融, 外汇, 农业, 工商业, 失业率, 旅游业,\n",
    "Average loss at step  14662000 :  3.75904047251 : 8647233\n",
    "Average loss at step  14664000 :  3.7323171851 : 9434064\n",
    "Average loss at step  14666000 :  3.75201312006 : 10209009\n",
    "Average loss at step  14668000 :  3.76831263733 : 10994219\n",
    "Average loss at step  14670000 :  3.68932747376 : 11780899\n",
    "Nearest to 音乐: 流行音乐, 作曲家, 古典音乐, 唱片, 音乐家, 音乐创作, 乐曲, 歌唱,\n",
    "Nearest to 地方: 地区, 省份, 区域, 城市, 巡查, 县内, 首长, 各地,\n",
    "Nearest to 社会: 知识分子, 价值观, 道德, 社会秩序, 社会主义, 贫困, 社会学, 家庭,\n",
    "Nearest to 服务: 运营, 业务, 营运, 邮件, 长途, 营办, 工作, 银行业,\n",
    "Nearest to 均: 皆, 都, 均会, 分别, 亦, 仍, 中均, 一律,\n",
    "Nearest to 型: W型, 千赫, 形, 式, 辆, /, 号车, 倍径,\n",
    "Nearest to 学生: 同学, 师生, 大学生, 留学生, 考生, 毕业生, 入学, 老师,\n",
    "Nearest to 今: 今属, 治今, 今日, 现, 今天, 现今, 一说, 棋王,\n",
    "Nearest to 受到: 受, 深受, 遭受, 备受, 不受, 遭到, 饱受, 广受,\n",
    "Nearest to 事件: 事故, 惨案, 事情, 爆炸案, 案件, 空难, 恐怖袭击, 事变,\n",
    "Nearest to 经济: 国民经济, 金融, 经济学, 外汇, 工商业, 农业, 失业率, 旅游业,\n",
    "Average loss at step  14672000 :  3.72136128151 : 12567280\n",
    "Average loss at step  14674000 :  3.73909988093 : 13351234\n",
    "Average loss at step  14676000 :  3.8108441782 : 14141637\n",
    "Average loss at step  14678000 :  3.78373070538 : 14932012\n",
    "Average loss at step  14680000 :  3.78765902841 : 15722407\n",
    "Nearest to 音乐: 流行音乐, 作曲家, 古典音乐, 音乐家, 唱片, 音乐创作, 乐曲, 旋律,\n",
    "Nearest to 地方: 地区, 省份, 区域, 巡查, 城市, 县内, 首长, 离岛,\n",
    "Nearest to 社会: 知识分子, 价值观, 道德, 社会主义, 社会秩序, 贫困, 心理, 社会学,\n",
    "Nearest to 服务: 运营, 业务, 营运, 邮件, 长途, 营办, 银行业, 邮政局,\n",
    "Nearest to 均: 皆, 都, 均会, 分别, 仍, 亦, 中均, 一律,\n",
    "Nearest to 型: W型, 千赫, 形, 辆, 式, 倍径, 号车, 分及,\n",
    "Nearest to 学生: 同学, 师生, 大学生, 留学生, 考生, 毕业生, 老师, 入学,\n",
    "Nearest to 今: 今属, 治今, 今日, 现, 今天, 现今, 零用钱, 泉源,\n",
    "Nearest to 受到: 受, 深受, 遭受, 备受, 不受, 遭到, 饱受, 广受,\n",
    "Nearest to 事件: 惨案, 事故, 事情, 爆炸案, 案件, 空难, 恐怖袭击, 事变,\n",
    "Nearest to 经济: 国民经济, 金融, 经济学, 失业率, 外汇, 工商业, 农业, 旅游业,\n",
    "Average loss at step  14682000 :  3.80160304058 : 16506014\n",
    "Average loss at step  14684000 :  3.80214735651 : 17296298\n",
    "Average loss at step  14686000 :  3.78929483211 : 18081963\n",
    "Average loss at step  14688000 :  3.79690062213 : 18875266\n",
    "Average loss at step  14690000 :  3.78898674619 : 19653927\n",
    "Nearest to 音乐: 流行音乐, 作曲家, 古典音乐, 音乐家, 音乐创作, 唱片, 乐曲, 旋律,\n",
    "Nearest to 地方: 地区, 省份, 区域, 城市, 巡查, 县内, 首长, 各地,\n",
    "Nearest to 社会: 价值观, 知识分子, 道德, 社会主义, 社会秩序, 贫困, 社会学, 心理,\n",
    "Nearest to 服务: 运营, 业务, 营运, 邮件, 长途, 营办, 工作, 银行业,\n",
    "Nearest to 均: 皆, 都, 均会, 分别, 仍, 亦, 中均, 一律,\n",
    "Nearest to 型: W型, 千赫, 形, -, 式, 辆, 分及, 倍径,\n",
    "Nearest to 学生: 同学, 师生, 大学生, 留学生, 考生, 毕业生, 老师, 入学,\n",
    "Nearest to 今: 今属, 治今, 今日, 现, 今天, 现今, 泉源, 零用钱,\n",
    "Nearest to 受到: 受, 深受, 遭受, 备受, 不受, 遭到, 饱受, 广受,\n",
    "Nearest to 事件: 事故, 惨案, 事情, 爆炸案, 案件, 空难, 恐怖袭击, 事变,\n",
    "Nearest to 经济: 国民经济, 金融, 经济学, 工商业, 外汇, 失业率, 农业, 经济学家,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_dir = \"./log_002_baseline_cbow/\"\n",
    "log_dir = \"./log_002_tmp/\"\n",
    "\n",
    "tfconfig = tf.ConfigProto()\n",
    "tfconfig.gpu_options.allow_growth = True\n",
    "# tfconfig.device_count = {'GPU': 0}\n",
    "\n",
    "with tf.Session(graph=graph, config=tfconfig) as session:\n",
    "    saver.restore(session, tf.train.latest_checkpoint(log_dir))\n",
    "    print('Initialized')\n",
    "    word2vec = embeddings.eval()\n",
    "    print(word2vec.shape, type(word2vec))\n",
    "    np.save(\"result/002#word_embedding_win5\", word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Evaluation - wordsim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordsim_240 = None\n",
    "with open(\"./data/240.txt\") as f:\n",
    "    wordsim_240 = f.readlines()\n",
    "    wordsim_240 = [line.strip().split(\"\\t\") for line in wordsim_240]\n",
    "wordsim_240[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordsim_297 = None\n",
    "with open(\"./data/297.txt\") as f:\n",
    "    wordsim_297 = f.readlines()\n",
    "    wordsim_297 = [line.strip().split(\"\\t\") for line in wordsim_297]\n",
    "wordsim_297[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log_dir = \"./log_002_baseline_cbow/\"\n",
    "\n",
    "# tfconfig = tf.ConfigProto(device_count={'gpu':0})\n",
    "tfconfig = tf.ConfigProto()\n",
    "tfconfig.gpu_options.allow_growth = True\n",
    "# tfconfig.device_count = {'GPU': 0}\n",
    "\n",
    "with tf.Session(graph=graph, config=tfconfig) as session:\n",
    "    saver.restore(session, tf.train.latest_checkpoint(log_dir))\n",
    "    print('Initialized')\n",
    "    context_lt = []\n",
    "    for item1, item2, score in wordsim_240:\n",
    "        if item1 not in word_to_id:\n",
    "            print(item1, item2)\n",
    "            continue\n",
    "        if item2 not in word_to_id:\n",
    "            print(item2)\n",
    "            continue\n",
    "        testitem1 = word_to_id[item1]\n",
    "        testitem2 = word_to_id[item2]\n",
    "        sim = similarity_smi_test.eval({test_dataset:[testitem1, testitem2]})\n",
    "        context_lt.append([item1, item2, sim[0][0]*10, float(score)])\n",
    "        print()\n",
    "        print(item1, item2, sim[0][0]*10, score)\n",
    "        index, value = session.run([similarity_test_top_k_index, similarity_test_top_k_value], feed_dict={test_dataset:[testitem1]})\n",
    "        for i, v in zip(index[0], value[0]):\n",
    "            print(id_to_word[i], v, end=\",\")\n",
    "        print()\n",
    "        index, value = session.run([similarity_test_top_k_index, similarity_test_top_k_value], feed_dict={test_dataset:[testitem2]})\n",
    "        for i, v in zip(index[0], value[0]):\n",
    "            print(id_to_word[i], v, end=\",\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_dir = \"./log_002_baseline_cbow/\"\n",
    "\n",
    "# tfconfig = tf.ConfigProto(device_count={'gpu':0})\n",
    "tfconfig = tf.ConfigProto()\n",
    "tfconfig.gpu_options.allow_growth = True\n",
    "# tfconfig.device_count = {'GPU': 0}\n",
    "\n",
    "with tf.Session(graph=graph, config=tfconfig) as session:\n",
    "    saver.restore(session, tf.train.latest_checkpoint(log_dir))\n",
    "    print('Initialized')\n",
    "    context_lt = []\n",
    "    for item1, item2, score in wordsim_297:\n",
    "        if item1 not in word_to_id:\n",
    "            print(item1, item2)\n",
    "            continue\n",
    "        if item2 not in word_to_id:\n",
    "            print(item2)\n",
    "            continue\n",
    "        testitem1 = word_to_id[item1]\n",
    "        testitem2 = word_to_id[item2]\n",
    "        sim = similarity_smi_test.eval({test_dataset:[testitem1, testitem2]})\n",
    "        context_lt.append([item1, item2, sim[0][0], float(score)])\n",
    "#         print(item1, item2, sim[0][0], score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# word_count['黄瓜']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "context_pd = pd.DataFrame(context_lt, columns=['a', 'b', 'pscore', 'score'])\n",
    "# context_pd = context_pd[['pscore', 'score']]\n",
    "context_pd['pscore'] = [float(line) for line in context_pd['pscore'].values]\n",
    "context_pd['score'] = [float(line) for line in context_pd['score'].values]\n",
    "context_pd.corr(\"spearman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "context_pd = pd.DataFrame(context_lt, columns=['a', 'b', 'pscore', 'score'])\n",
    "# context_pd = context_pd[['pscore', 'score']]\n",
    "context_pd['pscore'] = [float(line*5) for line in context_pd['pscore'].values]\n",
    "context_pd['score'] = [float(line) for line in context_pd['score'].values]\n",
    "context_pd.corr(\"spearman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "context_pd.to_csv(\"result/002#wordsim-296-0.547964.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "context_pd.to_csv(\"result/002#wordsim-240-0.462907.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Evaluation - analogy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# batch_size = 100\n",
    "# # batch_size = 256\n",
    "# skip_window = 5    # How many words to consider left and right.\n",
    "# num_skips = 2*skip_window    # How many times to reuse an input to generate a label.\n",
    "# num_sampled = 100    # Number of negative examples to sample.\n",
    "# # num_sampled = 128    # Number of negative examples to sample.\n",
    "\n",
    "embedding_size = 100    # Dimension of the embedding vector.\n",
    "vocabulary_size = len(id_to_word)\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    with tf.name_scope('inputs'):\n",
    "        test_dataset = tf.placeholder(tf.int32, shape=None)\n",
    "        \n",
    "    # Look up embeddings for inputs.\n",
    "    with tf.name_scope('embeddings'):\n",
    "        embeddings = tf.Variable(\n",
    "                tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "\n",
    "    # Compute the cosine similarity between minibatch examples and all embeddings.\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    \n",
    "    ### analogical reasoning\n",
    "    print(\"---- analogical reasoning\")\n",
    "    test_embeddings = tf.nn.embedding_lookup(embeddings, test_dataset)\n",
    "    print(\"test_embeddings:\", test_embeddings)\n",
    "    test_emb1, test_emb2, test_emb3, test_emb4 = tf.split(test_embeddings, [1, 1, 1, 1], 0)\n",
    "    print(\"test_embs:\", test_emb1, test_emb2)\n",
    "    print(\"test_embs:\", test_emb3, test_emb4)\n",
    "#     test_result_tmp = tf.subtract(test_emb1, test_emb2)\n",
    "    test_result = tf.add(tf.subtract(test_emb2, test_emb1), test_emb3)\n",
    "#     assert_sum_1_shape = tf.shape(test_result)\n",
    "#     assert_sum_1 = tf.reduce_sum(test_result, axis=-1)\n",
    "    analogical_product = tf.matmul(test_result, test_emb4, transpose_b=True)\n",
    "    analogical_norm_1 = tf.sqrt(tf.reduce_sum(tf.square(test_result), axis=-1))\n",
    "    analogical_norm_2 = tf.sqrt(tf.reduce_sum(tf.square(test_emb4), axis=-1))\n",
    "    analogical_smi_test = analogical_product / (analogical_norm_1 * analogical_norm_2)\n",
    "    print(\"test_result\", test_result)\n",
    "    print(\"normalized_embeddings\", normalized_embeddings)\n",
    "#     analogical_similarity = tf.squeeze(tf.matmul(tf.reshape(test_result, [1,-1]), normalized_embeddings, transpose_b=True))\n",
    "    test_result_norm = test_result / analogical_norm_1\n",
    "    analogical_similarity = tf.squeeze(tf.matmul(test_result_norm, normalized_embeddings, transpose_b=True))\n",
    "    print(\"analogical_similarity\", analogical_similarity)\n",
    "    analogical_top_k_value, analogical_top_k_index  = tf.nn.top_k(tf.squeeze(analogical_similarity), k=5)\n",
    "    print(\"analogical_top_k_value:\", analogical_top_k_value)\n",
    "    print(\"analogical_top_k_index:\", analogical_top_k_index)\n",
    "\n",
    "    # Merge all summaries.\n",
    "    merged = tf.summary.merge_all()\n",
    "\n",
    "    # Add variable initializer.\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Create a saver.\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_dir = \"./log_002_baseline_cbow/\"\n",
    "log_dir = \"./log_002_tmp/\"\n",
    "\n",
    "tfconfig = tf.ConfigProto()\n",
    "tfconfig.gpu_options.allow_growth = True\n",
    "# tfconfig.device_count = {'GPU': 0}\n",
    "\n",
    "with tf.Session(graph=graph, config=tfconfig) as session:\n",
    "    saver.restore(session, tf.train.latest_checkpoint(log_dir))\n",
    "    print('Initialized')\n",
    "    word2vec = embeddings.eval()\n",
    "    print(word2vec.shape, type(word2vec))\n",
    "    np.save(\"result/002#word_embedding_win20\", word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "analogy_1127 = None\n",
    "with open(\"./data/analogy-1127.txt\") as f:\n",
    "    analogy_1127 = f.readlines()\n",
    "    analogy_1127 = [line.strip().split() for line in analogy_1127]\n",
    "# analogy_1127[:5]\n",
    "\n",
    "analogy_1127_capital = []\n",
    "analogy_1127_state = []\n",
    "analogy_1127_family = []\n",
    "flag = 0\n",
    "for item in analogy_1127:\n",
    "    if item[0] == \":\":\n",
    "        flag += 1\n",
    "        analogy_1127.remove(item)\n",
    "        continue\n",
    "    if flag == 1:\n",
    "        analogy_1127_capital.append(item)\n",
    "    elif flag == 2:\n",
    "        analogy_1127_state.append(item)\n",
    "    elif flag == 3:\n",
    "        analogy_1127_family.append(item)\n",
    "analogy_1127_capital[:5], analogy_1127_state[:5], analogy_1127_family[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for line in analogy_1127:\n",
    "    print(line)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log_dir = \"./log_002_baseline_cbow/\"\n",
    "\n",
    "# tfconfig = tf.ConfigProto(device_count={'gpu':0})\n",
    "tfconfig = tf.ConfigProto()\n",
    "tfconfig.gpu_options.allow_growth = True\n",
    "# tfconfig.device_count = {'GPU': 0}\n",
    "\n",
    "with tf.Session(graph=graph, config=tfconfig) as session:\n",
    "    saver.restore(session, tf.train.latest_checkpoint(log_dir))\n",
    "    print('Initialized')\n",
    "    context_lt = []\n",
    "    count = 0\n",
    "    for item1, item2, item3, item4 in tqdm(analogy_1127):\n",
    "        if item1 not in word_to_id or\\\n",
    "            item2 not in word_to_id or\\\n",
    "            item3 not in word_to_id or\\\n",
    "            item4 not in word_to_id:\n",
    "            print(item1, item2, item3, item4)\n",
    "            continue\n",
    "        testitem1 = word_to_id[item1]\n",
    "        testitem2 = word_to_id[item2]\n",
    "        testitem3 = word_to_id[item3]\n",
    "        testitem4 = word_to_id[item4]\n",
    "        sim = analogical_smi_test.eval({test_dataset:[testitem1, testitem2, testitem3, testitem4]})\n",
    "#         assert_sum = assert_sum_1.eval({test_dataset:[testitem1, testitem2, testitem3, testitem4]})\n",
    "#         assert_sum_shape = assert_sum_1_shape.eval({test_dataset:[testitem1, testitem2, testitem3, testitem4]})\n",
    "#         assert_val = test_result.eval({test_dataset:[testitem1, testitem2, testitem3, testitem4]})\n",
    "#         assert_tmp_val = test_result_tmp.eval({test_dataset:[testitem1, testitem2, testitem3, testitem4]})\n",
    "#         test_emb1_val = test_emb1.eval({test_dataset:[testitem1, testitem2, testitem3, testitem4]})\n",
    "#         test_emb2_val = test_emb1.eval({test_dataset:[testitem1, testitem2, testitem3, testitem4]})\n",
    "#         test_emb3_val = test_emb1.eval({test_dataset:[testitem1, testitem2, testitem3, testitem4]})\n",
    "#         test_emb4_val = test_emb1.eval({test_dataset:[testitem1, testitem2, testitem3, testitem4]})\n",
    "#         test_embs_val = test_embeddings.eval({test_dataset:[testitem1, testitem2, testitem3, testitem4]})\n",
    "#         voca_embs = normalized_embeddings.eval()\n",
    "#         print(assert_sum, assert_sum_shape)\n",
    "#         print(assert_val)\n",
    "#         print(assert_tmp_val)\n",
    "        \n",
    "#         print(item1, item2, item3, item4, sim[0][0])\n",
    "        value, index  = session.run([analogical_top_k_value, analogical_top_k_index], feed_dict={test_dataset:[testitem1, testitem2, testitem3, testitem4]})\n",
    "        \n",
    "#         print(index, value)\n",
    "        sim_item = []\n",
    "        for i, v in zip(index, value):\n",
    "#             print(id_to_word[i], v, end=\",\")\n",
    "            sim_item.append(id_to_word[i] + str(v))\n",
    "#         print()\n",
    "        if sim[0][0] >= value[1]: count += 1\n",
    "        context_lt.append([item1, item2, item3, item4, sim[0][0]])\n",
    "        context_lt[-1].extend(sim_item)\n",
    "    print(\"accuracy:\", count/len(analogy_1127)*100.0, \"%\")\n",
    "#         print(item1, item2, sim[0][0], score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(context_lt).to_csv(\"result/002#analogy_1127_0.0907.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
