{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å¯¹wikiä¸­æ–‡è¯åº“æ•°æ®çš„å¯¼å‡º\n",
    "- ç›®æ ‡ï¼šé˜²æ­¢å†…å­˜è¿‡åº¦ä½¿ç”¨\n",
    "- å¯¼å‡ºæ•°æ®åŒ…æ‹¬ï¼š\n",
    "  - zh_wiki_idï¼šidè¡¨ç¤ºçš„ä¸­æ–‡å•è¯wikiæ–‡ç« ï¼Œä¸€è¡Œ\n",
    "  - word_to_id.pklï¼šä¸­æ–‡å•è¯åˆ°idçš„æ˜ å°„è¯å…¸\n",
    "  - id_to_word.pklï¼šidåˆ°ä¸­æ–‡å•è¯çš„æ˜ å°„è¯å…¸\n",
    "  - word_count.pklï¼šä¸­æ–‡å•è¯çš„å‡ºç°æ¬¡æ•°çš„è¯å…¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import random\n",
    "from tempfile import gettempdir\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tensorflow.contrib.tensorboard.plugins import projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    \"\"\"Extract the first file enclosed in a zip file as a list of words.\"\"\"\n",
    "    with open(filename) as f:\n",
    "        data = f.readlines()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4056734/4056734 [00:32<00:00, 126585.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 222822535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data = read_data(\"./data/zh_wiki_word_w_d\")\n",
    "vocabulary = []\n",
    "for line in tqdm(data):\n",
    "    line = line.strip()\n",
    "    for item in line.split():\n",
    "#         print(item)\n",
    "        vocabulary.append(item)\n",
    "#     break\n",
    "# vocabulary = [v for v in vocabulary]\n",
    "print('Data size', len(vocabulary))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "zh_wiki_word: 222822535 / line num: 4056734\n",
    "zh_wiki_word_w_d: 183765886 / line num: 4056734"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åš å¤´å‘ æ˜¯ ä¸­å›½ å¤§é™† äº’è”ç½‘ æµè¡Œè¯­ ä¹‹ä¸€ ï¼Œ è¯¥ è¯æ±‡ å‘æº äº D å¹´ D æœˆ D æ—¥ ä¸­å›½ å¤§é™† å¨±ä¹åœˆ ä¸€ä¸ª çˆ†ç‚¸ æ–°é—» â€” â€” æå°ç’ å‡ºè½¨ äº‹ä»¶ ï¼Œ å½“å¤© å½“ æ°‘ä¼— éƒ½ åœ¨ å…³æ³¨ å„å¤§ å«è§† è·¨å¹´ æ™šä¼š æ—¶ ï¼Œ æå°ç’ åœ¨ W å®¶ä¸­ è¿‡å¤œ çš„ è§†é¢‘ ç«é ä¸­å›½ å¨±ä¹åœˆ ï¼Œ è€Œ è´¾ä¹ƒäº® å½“æ™š ç›´æ’­ è¯´ æå°ç’ å» åš å¤´å‘ äº† ï¼Œ æ‰€ä»¥ ä¸ åœ¨å®¶ ã€‚ â€œ åš å¤´å‘ â€ æ¢— å°±æ­¤ è€Œ æ¥ ï¼Œ æˆä¸º ç»§ å¨±ä¹åœˆ æ˜æ˜Ÿ ä»¬ æ™šä¸Š å…³é—¨ èŠ å‰§æœ¬ ä¹‹å çš„ åˆ ä¸€ å‡ºè½¨ éšå–» ã€‚\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ã€‚'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data[-1])\n",
    "del data\n",
    "vocabulary[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['æ•°å­¦', 'æ˜¯', 'åˆ©ç”¨', 'ç¬¦å·è¯­è¨€', 'ç ”ç©¶', 'æ•°é‡', 'ç»“æ„', 'å˜åŒ–', 'ä»¥åŠ', 'ç©ºé—´']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2041017"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('é›·ç“¦ä¹¡', 1),\n",
       " ('æ˜Ÿæ£±å¤§', 1),\n",
       " ('ç©ç½‘', 1),\n",
       " ('ğŸ’§', 1),\n",
       " ('å……æµ©æ˜¯', 1),\n",
       " ('æˆ‘é¸¡å“¥', 1),\n",
       " ('äººç‹ è¯', 1),\n",
       " ('çº¹ç¼˜', 1),\n",
       " ('çŒ¥çç”·', 1),\n",
       " ('æ™®æ³›', 1)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_cnt = collections.Counter(vocabulary).most_common(2041017)\n",
    "word_cnt[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191551\n"
     ]
    }
   ],
   "source": [
    "for index, (k,v) in enumerate(word_cnt):\n",
    "    if v <= 25:\n",
    "        print(index)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"data/word_count_w_d.pkl\", \"wb\") as f:\n",
    "    pickle.dump(word_cnt, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('é²æ´ªå‡', 5), ('èµ«åˆ©çº³', 5), ('ä½†æ³½å·', 5), ('é©å…±', 5), ('æŸ¯å°š', 4)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "look_num = -2740\n",
    "word_cnt[:look_num][-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary_size = 504175\n",
    "vocabulary_size = 510000 - 2740\n",
    "\n",
    "def build_dataset(words, n_words):\n",
    "    \"\"\"Process raw inputs into a dataset.\"\"\"\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(n_words - 1))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        index = dictionary.get(word, 0)\n",
    "        if index == 0:  # dictionary['UNK']\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reversed_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words (+UNK) [['UNK', 2297816], ('ï¼Œ', 14808459), ('çš„', 9437707), ('ã€‚', 8706775), ('D', 8592359)]\n",
      "Sample data [1348, 9, 501, 237319, 141, 894, 5, 499, 5, 331] ['æ•°å­¦', 'æ˜¯', 'åˆ©ç”¨', 'ç¬¦å·è¯­è¨€', 'ç ”ç©¶', 'æ•°é‡', 'ã€', 'ç»“æ„', 'ã€', 'å˜åŒ–']\n"
     ]
    }
   ],
   "source": [
    "# Filling 4 global variables:\n",
    "# data - list of codes (integers from 0 to vocabulary_size-1).\n",
    "#   This is the original text but words are replaced by their codes\n",
    "# count - map of words(strings) to count of occurrences\n",
    "# dictionary - map of words(strings) to their codes(integers)\n",
    "# reverse_dictionary - maps codes(integers) to words(strings)\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(vocabulary, vocabulary_size)\n",
    "del vocabulary  # Hint to reduce memory.\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Sample data', data[:10], [reverse_dictionary[i] for i in data[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Most common words (+UNK) [['UNK', 2291206], ('çš„', 9437707), ('å¹´', 2807078), ('åœ¨', 2679806), ('æ˜¯', 2112111)]\n",
    "Sample data [1296, 4, 462, 235572, 116, 850, 460, 301, 53, 611] ['æ•°å­¦', 'æ˜¯', 'åˆ©ç”¨', 'ç¬¦å·è¯­è¨€', 'ç ”ç©¶', 'æ•°é‡', 'ç»“æ„', 'å˜åŒ–', 'ä»¥åŠ', 'ç©ºé—´']\n",
    "\n",
    "Most common words (+UNK) [['UNK', 2297816], ('ï¼Œ', 14808459), ('çš„', 9437707), ('ã€‚', 8706775), ('D', 8592359)]\n",
    "Sample data [1348, 9, 501, 237319, 141, 894, 5, 499, 5, 331] ['æ•°å­¦', 'æ˜¯', 'åˆ©ç”¨', 'ç¬¦å·è¯­è¨€', 'ç ”ç©¶', 'æ•°é‡', 'ã€', 'ç»“æ„', 'ã€', 'å˜åŒ–']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('éƒç¼', 5),\n",
       " ('çº¿å¤å‘', 5),\n",
       " ('å°¼ä¹¡', 5),\n",
       " ('æ³°ä»€è’‚ä¹¡', 5),\n",
       " ('æ¢…æ–', 5),\n",
       " ('å¢å¾·å§†', 5),\n",
       " ('é²æ´ªå‡', 5),\n",
       " ('èµ«åˆ©çº³', 5),\n",
       " ('ä½†æ³½å·', 5),\n",
       " ('é©å…±', 5)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"data/zh_wiki_id_w_d\", \"w\") as f:\n",
    "    context = [str(i) for i in data]\n",
    "    f.write(\" \".join(context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"data/word_to_id_w_d.pkl\", \"wb\") as f:\n",
    "    pickle.dump(dictionary, f)\n",
    "with open(\"data/id_to_word_w_d.pkl\", \"wb\") as f:\n",
    "    pickle.dump(reverse_dictionary, f)\n",
    "# with open(\"data/word_count_w_d.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(count, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Load Data Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zh_wiki_id = open(\"data/zh_wiki_id\").readline()\n",
    "word_to_id = pickle.load(open(\"data/word_to_id.pkl\", \"rb\"))\n",
    "id_to_word = pickle.load(open(\"data/id_to_word.pkl\", \"rb\"))\n",
    "word_count = pickle.load(open(\"data/count.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zh_wiki_id = open(\"data/zh_wiki_id_w_d\").readline()\n",
    "word_to_id = pickle.load(open(\"data/word_to_id_w_d.pkl\", \"rb\"))\n",
    "id_to_word = pickle.load(open(\"data/id_to_word_w_d.pkl\", \"rb\"))\n",
    "# word_count = pickle.load(open(\"data/count.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "507260"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getWord(data, num, data_index):\n",
    "    sub_data_string = data[data_index:data_index+num*(6+1)]\n",
    "    print(sub_data_string)\n",
    "    result = []\n",
    "    for index, item in enumerate(sub_data_string.split()):\n",
    "        if index == num: break\n",
    "        data_index += len(item) + 1\n",
    "        result.append(int(item))\n",
    "    if len(result) < num:\n",
    "        return getWord(data, num, 0)\n",
    "    assert len(result) == num\n",
    "    return result, data_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "727934301"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(zh_wiki_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21556\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([21556], 727934302)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getWord(zh_wiki_id, 1, 727934301-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_batch(batch_size, skip_window, num_skips):\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size, num_skips), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1  # [ skip_window target skip_window ]\n",
    "    assert batch_size >= span\n",
    "    buffer = collections.deque(maxlen=span)  # pylint: disable=redefined-builtin\n",
    "#     if data_index + span*(6+1) > len(data):\n",
    "#         data_index = 0\n",
    "    result, data_index = getWord(zh_wiki_id, span, data_index)\n",
    "    buffer.extend(result)\n",
    "#     data_index += span\n",
    "    for i in range(batch_size):\n",
    "#         print(data_index, data_index + span, buffer)\n",
    "        context_words = [w for w in range(span) if w != skip_window]\n",
    "#         words_to_use = random.sample(context_words, num_skips)\n",
    "        # context tokens are just all the tokens in buffer except the target\n",
    "#         batch[i, :] = [token for idx, token in enumerate(buffer) if idx != context_window]\n",
    "#         batch[i, :] = [buffer[token] for idx, token in enumerate(words_to_use)]\n",
    "        batch[i, :] = [buffer[token] for idx, token in enumerate(context_words)]\n",
    "        labels[i, 0] = buffer[skip_window]\n",
    "        result, data_index = getWord(zh_wiki_id, 1, data_index)\n",
    "        buffer.append(result[0])\n",
    "        if data_index > len(zh_wiki_id):\n",
    "            result, data_index = getWord(zh_wiki_id, span-1, 0)\n",
    "            buffer.extend(result)\n",
    "        if i == batch_size - span:\n",
    "            last_index = data_index\n",
    "#         data_index = (data_index + 1) % len(data)\n",
    "        \n",
    "#         print(\"batch:\", [i for i in batch], \"\\nlabels:\", [ j for i in labels for j in i])\n",
    "#         print(\"batch:\", batch[i], \"\\nlabels:\", labels[i])\n",
    "    # Backtrack a little bit to avoid skipping words in the end of a batch\n",
    "#     data_index = (data_index + len(data) - span) % len(data)\n",
    "    data_index = last_index\n",
    "#     print(\"batch:\", batch, \"\\nlabels:\", labels)\n",
    "    return batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73 68 7400 2 8354 41 690 23 262 271 9578 10038 190 2 3132 3 607 1348 2 1908 1\n",
      "10038 1\n",
      "190 2 3\n",
      "2 3132 \n",
      "3132 3 \n",
      "3 607 1\n",
      "607 134\n",
      "1348 2 \n",
      "2 1908 \n",
      "1908 18\n",
      "18 1677\n",
      "1677 34\n",
      "3419 55\n",
      "550 18 \n",
      "18 680 \n",
      "680 253\n",
      "253 23 \n",
      "23 1718\n",
      "17185 2\n",
      "2 17826\n",
      "17826 3\n",
      "318\n",
      "batchï¼š (20, 10)\n",
      "labelsï¼š (20, 1)\n",
      "73 ä»¥åŠ 68 ä» 7400 é€‰å®š 2 çš„ 8354 å…¬ç† 690 å®šä¹‰ 23 ä¸­ 262 å»ºç«‹ 271 èµ· 9578 ä¸¥è°¨ -> 41 åŠ\n",
      "68 ä» 7400 é€‰å®š 2 çš„ 8354 å…¬ç† 41 åŠ 23 ä¸­ 262 å»ºç«‹ 271 èµ· 9578 ä¸¥è°¨ 10038 æ¨å¯¼ -> 690 å®šä¹‰\n",
      "7400 é€‰å®š 2 çš„ 8354 å…¬ç† 41 åŠ 690 å®šä¹‰ 262 å»ºç«‹ 271 èµ· 9578 ä¸¥è°¨ 10038 æ¨å¯¼ 190 å‡º -> 23 ä¸­\n",
      "2 çš„ 8354 å…¬ç† 41 åŠ 690 å®šä¹‰ 23 ä¸­ 271 èµ· 9578 ä¸¥è°¨ 10038 æ¨å¯¼ 190 å‡º 2 çš„ -> 262 å»ºç«‹\n",
      "8354 å…¬ç† 41 åŠ 690 å®šä¹‰ 23 ä¸­ 262 å»ºç«‹ 9578 ä¸¥è°¨ 10038 æ¨å¯¼ 190 å‡º 2 çš„ 3132 å®šç† -> 271 èµ·\n",
      "41 åŠ 690 å®šä¹‰ 23 ä¸­ 262 å»ºç«‹ 271 èµ· 10038 æ¨å¯¼ 190 å‡º 2 çš„ 3132 å®šç† 3 ã€‚ -> 9578 ä¸¥è°¨\n",
      "690 å®šä¹‰ 23 ä¸­ 262 å»ºç«‹ 271 èµ· 9578 ä¸¥è°¨ 190 å‡º 2 çš„ 3132 å®šç† 3 ã€‚ 607 åŸºç¡€ -> 10038 æ¨å¯¼\n",
      "23 ä¸­ 262 å»ºç«‹ 271 èµ· 9578 ä¸¥è°¨ 10038 æ¨å¯¼ 2 çš„ 3132 å®šç† 3 ã€‚ 607 åŸºç¡€ 1348 æ•°å­¦ -> 190 å‡º\n",
      "262 å»ºç«‹ 271 èµ· 9578 ä¸¥è°¨ 10038 æ¨å¯¼ 190 å‡º 3132 å®šç† 3 ã€‚ 607 åŸºç¡€ 1348 æ•°å­¦ 2 çš„ -> 2 çš„\n",
      "271 èµ· 9578 ä¸¥è°¨ 10038 æ¨å¯¼ 190 å‡º 2 çš„ 3 ã€‚ 607 åŸºç¡€ 1348 æ•°å­¦ 2 çš„ 1908 çŸ¥è¯† -> 3132 å®šç†\n",
      "9578 ä¸¥è°¨ 10038 æ¨å¯¼ 190 å‡º 2 çš„ 3132 å®šç† 607 åŸºç¡€ 1348 æ•°å­¦ 2 çš„ 1908 çŸ¥è¯† 18 ä¸ -> 3 ã€‚\n",
      "10038 æ¨å¯¼ 190 å‡º 2 çš„ 3132 å®šç† 3 ã€‚ 1348 æ•°å­¦ 2 çš„ 1908 çŸ¥è¯† 18 ä¸ 1677 è¿ç”¨ -> 607 åŸºç¡€\n",
      "190 å‡º 2 çš„ 3132 å®šç† 3 ã€‚ 607 åŸºç¡€ 2 çš„ 1908 çŸ¥è¯† 18 ä¸ 1677 è¿ç”¨ 3419 æ€»æ˜¯ -> 1348 æ•°å­¦\n",
      "2 çš„ 3132 å®šç† 3 ã€‚ 607 åŸºç¡€ 1348 æ•°å­¦ 1908 çŸ¥è¯† 18 ä¸ 1677 è¿ç”¨ 3419 æ€»æ˜¯ 550 ä¸ªäºº -> 2 çš„\n",
      "3132 å®šç† 3 ã€‚ 607 åŸºç¡€ 1348 æ•°å­¦ 2 çš„ 18 ä¸ 1677 è¿ç”¨ 3419 æ€»æ˜¯ 550 ä¸ªäºº 18 ä¸ -> 1908 çŸ¥è¯†\n",
      "3 ã€‚ 607 åŸºç¡€ 1348 æ•°å­¦ 2 çš„ 1908 çŸ¥è¯† 1677 è¿ç”¨ 3419 æ€»æ˜¯ 550 ä¸ªäºº 18 ä¸ 680 å›¢ä½“ -> 18 ä¸\n",
      "607 åŸºç¡€ 1348 æ•°å­¦ 2 çš„ 1908 çŸ¥è¯† 18 ä¸ 3419 æ€»æ˜¯ 550 ä¸ªäºº 18 ä¸ 680 å›¢ä½“ 253 ç”Ÿæ´» -> 1677 è¿ç”¨\n",
      "1348 æ•°å­¦ 2 çš„ 1908 çŸ¥è¯† 18 ä¸ 1677 è¿ç”¨ 550 ä¸ªäºº 18 ä¸ 680 å›¢ä½“ 253 ç”Ÿæ´» 23 ä¸­ -> 3419 æ€»æ˜¯\n",
      "2 çš„ 1908 çŸ¥è¯† 18 ä¸ 1677 è¿ç”¨ 3419 æ€»æ˜¯ 18 ä¸ 680 å›¢ä½“ 253 ç”Ÿæ´» 23 ä¸­ 17185 ä¸å¯æˆ–ç¼º -> 550 ä¸ªäºº\n",
      "1908 çŸ¥è¯† 18 ä¸ 1677 è¿ç”¨ 3419 æ€»æ˜¯ 550 ä¸ªäºº 680 å›¢ä½“ 253 ç”Ÿæ´» 23 ä¸­ 17185 ä¸å¯æˆ–ç¼º 2 çš„ -> 18 ä¸\n"
     ]
    }
   ],
   "source": [
    "# data_index = 0\n",
    "\n",
    "batch, labels = generate_batch(batch_size=20, skip_window=5, num_skips=2*5)\n",
    "print(data_index)\n",
    "print(\"batchï¼š\", batch.shape)\n",
    "print(\"labelsï¼š\", labels.shape)\n",
    "for i in range(20):\n",
    "    print(batch[i, 0], id_to_word[batch[i, 0]],\n",
    "          batch[i, 1], id_to_word[batch[i, 1]],\n",
    "          batch[i, 2], id_to_word[batch[i, 2]],\n",
    "          batch[i, 3], id_to_word[batch[i, 3]],\n",
    "          batch[i, 4], id_to_word[batch[i, 4]],\n",
    "          batch[i, 5], id_to_word[batch[i, 5]],\n",
    "          batch[i, 6], id_to_word[batch[i, 6]],\n",
    "          batch[i, 7], id_to_word[batch[i, 7]],\n",
    "          batch[i, 8], id_to_word[batch[i, 8]],\n",
    "          batch[i, 9], id_to_word[batch[i, 9]],\n",
    "          '->', labels[i, 0], id_to_word[labels[i, 0]])\n",
    "# for i in range(8):\n",
    "#     print(batch[i], reverse_dictionary[batch[i]], '->', labels[i, 0],\n",
    "#         reverse_dictionary[labels[i, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 273 4340 294 6398 119 858 197 49216 77 1 11664 53 48 7268 1 8205 26 649 12\n",
      "11664 5\n",
      "53 48 7\n",
      "48 7268\n",
      "7268 1 \n",
      "1 8205 \n",
      "8205 26\n",
      "26 649 \n",
      "649 12 \n",
      "12 235 \n",
      "235 244\n",
      "244 942\n",
      "9420 98\n",
      "9874 16\n",
      "163 1 3\n",
      "1 3057 \n",
      "3057 56\n",
      "567 129\n",
      "1296 1 \n",
      "1 1851 \n",
      "1851 9 \n",
      "249\n",
      "batchï¼š (20, 10)\n",
      "labelsï¼š (20, 1)\n",
      "21 è€Œ 273 äº§ç”Ÿ 273 æ•°å­¦å®¶ 273 ä»¬ 273 æ‹“å±• 273 æ¦‚å¿µ 273 ä¸ºäº† 273 å…¬å¼åŒ– 273 æ–° 273 çš„ -> 119 è¿™äº›\n",
      "273 äº§ç”Ÿ 4340 æ•°å­¦å®¶ 4340 ä»¬ 4340 æ‹“å±• 4340 è¿™äº› 4340 ä¸ºäº† 4340 å…¬å¼åŒ– 4340 æ–° 4340 çš„ 4340 çŒœæƒ³ -> 858 æ¦‚å¿µ\n",
      "4340 æ•°å­¦å®¶ 294 ä»¬ 294 æ‹“å±• 294 è¿™äº› 294 æ¦‚å¿µ 294 å…¬å¼åŒ– 294 æ–° 294 çš„ 294 çŒœæƒ³ 294 ä»¥åŠ -> 197 ä¸ºäº†\n",
      "294 ä»¬ 6398 æ‹“å±• 6398 è¿™äº› 6398 æ¦‚å¿µ 6398 ä¸ºäº† 6398 æ–° 6398 çš„ 6398 çŒœæƒ³ 6398 ä»¥åŠ 6398 ä» -> 49216 å…¬å¼åŒ–\n",
      "6398 æ‹“å±• 119 è¿™äº› 119 æ¦‚å¿µ 119 ä¸ºäº† 119 å…¬å¼åŒ– 119 çš„ 119 çŒœæƒ³ 119 ä»¥åŠ 119 ä» 119 é€‰å®š -> 77 æ–°\n",
      "119 è¿™äº› 858 æ¦‚å¿µ 858 ä¸ºäº† 858 å…¬å¼åŒ– 858 æ–° 858 çŒœæƒ³ 858 ä»¥åŠ 858 ä» 858 é€‰å®š 858 çš„ -> 1 çš„\n",
      "858 æ¦‚å¿µ 197 ä¸ºäº† 197 å…¬å¼åŒ– 197 æ–° 197 çš„ 197 ä»¥åŠ 197 ä» 197 é€‰å®š 197 çš„ 197 å…¬ç† -> 11664 çŒœæƒ³\n",
      "197 ä¸ºäº† 49216 å…¬å¼åŒ– 49216 æ–° 49216 çš„ 49216 çŒœæƒ³ 49216 ä» 49216 é€‰å®š 49216 çš„ 49216 å…¬ç† 49216 åŠ -> 53 ä»¥åŠ\n",
      "49216 å…¬å¼åŒ– 77 æ–° 77 çš„ 77 çŒœæƒ³ 77 ä»¥åŠ 77 é€‰å®š 77 çš„ 77 å…¬ç† 77 åŠ 77 å®šä¹‰ -> 48 ä»\n",
      "77 æ–° 1 çš„ 1 çŒœæƒ³ 1 ä»¥åŠ 1 ä» 1 çš„ 1 å…¬ç† 1 åŠ 1 å®šä¹‰ 1 ä¸­ -> 7268 é€‰å®š\n",
      "1 çš„ 11664 çŒœæƒ³ 11664 ä»¥åŠ 11664 ä» 11664 é€‰å®š 11664 å…¬ç† 11664 åŠ 11664 å®šä¹‰ 11664 ä¸­ 11664 å»ºç«‹ -> 1 çš„\n",
      "11664 çŒœæƒ³ 53 ä»¥åŠ 53 ä» 53 é€‰å®š 53 çš„ 53 åŠ 53 å®šä¹‰ 53 ä¸­ 53 å»ºç«‹ 53 èµ· -> 8205 å…¬ç†\n",
      "53 ä»¥åŠ 48 ä» 48 é€‰å®š 48 çš„ 48 å…¬ç† 48 å®šä¹‰ 48 ä¸­ 48 å»ºç«‹ 48 èµ· 48 ä¸¥è°¨ -> 26 åŠ\n",
      "48 ä» 7268 é€‰å®š 7268 çš„ 7268 å…¬ç† 7268 åŠ 7268 ä¸­ 7268 å»ºç«‹ 7268 èµ· 7268 ä¸¥è°¨ 7268 æ¨å¯¼ -> 649 å®šä¹‰\n",
      "7268 é€‰å®š 1 çš„ 1 å…¬ç† 1 åŠ 1 å®šä¹‰ 1 å»ºç«‹ 1 èµ· 1 ä¸¥è°¨ 1 æ¨å¯¼ 1 å‡º -> 12 ä¸­\n",
      "1 çš„ 8205 å…¬ç† 8205 åŠ 8205 å®šä¹‰ 8205 ä¸­ 8205 èµ· 8205 ä¸¥è°¨ 8205 æ¨å¯¼ 8205 å‡º 8205 çš„ -> 235 å»ºç«‹\n",
      "8205 å…¬ç† 26 åŠ 26 å®šä¹‰ 26 ä¸­ 26 å»ºç«‹ 26 ä¸¥è°¨ 26 æ¨å¯¼ 26 å‡º 26 çš„ 26 å®šç† -> 244 èµ·\n",
      "26 åŠ 649 å®šä¹‰ 649 ä¸­ 649 å»ºç«‹ 649 èµ· 649 æ¨å¯¼ 649 å‡º 649 çš„ 649 å®šç† 649 åŸºç¡€ -> 9420 ä¸¥è°¨\n",
      "649 å®šä¹‰ 12 ä¸­ 12 å»ºç«‹ 12 èµ· 12 ä¸¥è°¨ 12 å‡º 12 çš„ 12 å®šç† 12 åŸºç¡€ 12 æ•°å­¦ -> 9874 æ¨å¯¼\n",
      "12 ä¸­ 235 å»ºç«‹ 235 èµ· 235 ä¸¥è°¨ 235 æ¨å¯¼ 235 çš„ 235 å®šç† 235 åŸºç¡€ 235 æ•°å­¦ 235 çš„ -> 163 å‡º\n"
     ]
    }
   ],
   "source": [
    "# data_index = 0\n",
    "\n",
    "batch, labels = generate_batch(batch_size=20, skip_window=5, num_skips=2*5)\n",
    "print(data_index)\n",
    "print(\"batchï¼š\", batch.shape)\n",
    "print(\"labelsï¼š\", labels.shape)\n",
    "for i in range(20):\n",
    "    print(batch[i, 0], id_to_word[batch[i, 0]],\n",
    "          batch[i, 1], id_to_word[batch[i, 1]],\n",
    "          batch[i, 1], id_to_word[batch[i, 2]],\n",
    "          batch[i, 1], id_to_word[batch[i, 3]],\n",
    "          batch[i, 1], id_to_word[batch[i, 4]],\n",
    "          batch[i, 1], id_to_word[batch[i, 5]],\n",
    "          batch[i, 1], id_to_word[batch[i, 6]],\n",
    "          batch[i, 1], id_to_word[batch[i, 7]],\n",
    "          batch[i, 1], id_to_word[batch[i, 8]],\n",
    "          batch[i, 1], id_to_word[batch[i, 9]],\n",
    "          '->', labels[i, 0], id_to_word[labels[i, 0]])\n",
    "# for i in range(8):\n",
    "#     print(batch[i], reverse_dictionary[batch[i]], '->', labels[i, 0],\n",
    "#         reverse_dictionary[labels[i, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
