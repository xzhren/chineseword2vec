{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import random\n",
    "from tempfile import gettempdir\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pickle\n",
    "\n",
    "from tensorflow.contrib.tensorboard.plugins import projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zh_wiki_id = open(\"data/zh_wiki_id_w_d\").readline()\n",
    "word_to_id = pickle.load(open(\"data/word_to_id_w_d.pkl\", \"rb\"))\n",
    "id_to_word = pickle.load(open(\"data/id_to_word_w_d.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(507260, 507260, 862838467)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(id_to_word), len(word_to_id), len(zh_wiki_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getWord(data, num, data_index):\n",
    "    sub_data_string = data[data_index:data_index+num*(6+1)]\n",
    "    result = []\n",
    "    for index, item in enumerate(sub_data_string.split()):\n",
    "        if index == num: break\n",
    "        data_index += len(item) + 1\n",
    "        result.append(int(item))\n",
    "    if len(result) < num:\n",
    "        return getWord(data, num, 0)\n",
    "    assert len(result) == num\n",
    "    return result, data_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_batch(batch_size, skip_window, num_skips):\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size, num_skips), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1  # [ skip_window target skip_window ]\n",
    "    assert batch_size >= span\n",
    "    buffer = collections.deque(maxlen=span)  # pylint: disable=redefined-builti\n",
    "    \n",
    "    result, data_index = getWord(zh_wiki_id, span, data_index)\n",
    "    buffer.extend(result)\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        context_words = [w for w in range(span) if w != skip_window]\n",
    "        batch[i, :] = [buffer[token] for idx, token in enumerate(context_words)]\n",
    "        labels[i, 0] = buffer[skip_window]\n",
    "        result, data_index = getWord(zh_wiki_id, 1, data_index)\n",
    "        buffer.append(result[0])\n",
    "        if data_index > len(zh_wiki_id):\n",
    "            result, data_index = getWord(zh_wiki_id, span-1, 0)\n",
    "            buffer.extend(result)\n",
    "        if i == batch_size - span:\n",
    "            last_index = data_index\n",
    "            \n",
    "    # Backtrack a little bit to avoid skipping words in the end of a batch\n",
    "    data_index = last_index\n",
    "    return batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "331 变化 73 以及 -> 651 空间\n",
      "73 以及 651 空间 -> 38 等\n",
      "651 空间 38 等 -> 904 概念\n",
      "38 等 904 概念 -> 2 的\n",
      "904 概念 2 的 -> 5871 一门\n",
      "2 的 5871 一门 -> 2348 学科\n",
      "5871 一门 2348 学科 -> 1 ，\n",
      "2348 学科 1 ， -> 68 从\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data_index = 0\n",
    "\n",
    "batch, labels = generate_batch(batch_size=8, skip_window=2, num_skips=2*2)\n",
    "for i in range(8):\n",
    "    print(batch[i, 0], id_to_word[batch[i, 0]],\n",
    "          batch[i, 1], id_to_word[batch[i, 1]],\n",
    "          '->', labels[i, 0], id_to_word[labels[i, 0]])\n",
    "data_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_max_len = 16\n",
    "vocabulary_size = len(id_to_word)\n",
    "def get_char_batch(word_batch):\n",
    "    char_batch = []\n",
    "    char_num_batch = []\n",
    "    for win in word_batch:\n",
    "        char_win = []\n",
    "        char_num_win = []\n",
    "        for word in win:\n",
    "            char_win_w = [vocabulary_size] * word_max_len\n",
    "            w = id_to_word[word]\n",
    "            for i, c in enumerate(w):\n",
    "                if i >= word_max_len: break\n",
    "                if c in word_to_id:\n",
    "                    char_win_w[i] = word_to_id[c]\n",
    "            char_win.append(char_win_w)\n",
    "            char_num_win.append([len(w)])\n",
    "        char_batch.append(char_win)\n",
    "        char_num_batch.append(char_num_win)\n",
    "    return np.array(char_batch), np.array(char_num_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数 学 0 0 0 0 0 0 0 0 0 0 0 0 0 0  - 是 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0  - 符 号 语 言 0 0 0 0 0 0 0 0 0 0 0 0  - 研 究 0 0 0 0 0 0 0 0 0 0 0 0 0 0  - \n",
      "是 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0  - 利 用 0 0 0 0 0 0 0 0 0 0 0 0 0 0  - 研 究 0 0 0 0 0 0 0 0 0 0 0 0 0 0  - 数 量 0 0 0 0 0 0 0 0 0 0 0 0 0 0  - \n",
      "利 用 0 0 0 0 0 0 0 0 0 0 0 0 0 0  - 符 号 语 言 0 0 0 0 0 0 0 0 0 0 0 0  - 数 量 0 0 0 0 0 0 0 0 0 0 0 0 0 0  - 、 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0  - \n",
      "符 号 语 言 0 0 0 0 0 0 0 0 0 0 0 0  - 研 究 0 0 0 0 0 0 0 0 0 0 0 0 0 0  - 、 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0  - 结 构 0 0 0 0 0 0 0 0 0 0 0 0 0 0  - \n",
      "研 究 0 0 0 0 0 0 0 0 0 0 0 0 0 0  - 数 量 0 0 0 0 0 0 0 0 0 0 0 0 0 0  - 结 构 0 0 0 0 0 0 0 0 0 0 0 0 0 0  - 、 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0  - \n",
      "数 量 0 0 0 0 0 0 0 0 0 0 0 0 0 0  - 、 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0  - 、 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0  - 变 化 0 0 0 0 0 0 0 0 0 0 0 0 0 0  - \n",
      "、 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0  - 结 构 0 0 0 0 0 0 0 0 0 0 0 0 0 0  - 变 化 0 0 0 0 0 0 0 0 0 0 0 0 0 0  - 以 及 0 0 0 0 0 0 0 0 0 0 0 0 0 0  - \n",
      "结 构 0 0 0 0 0 0 0 0 0 0 0 0 0 0  - 、 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0  - 以 及 0 0 0 0 0 0 0 0 0 0 0 0 0 0  - 空 间 0 0 0 0 0 0 0 0 0 0 0 0 0 0  - \n"
     ]
    }
   ],
   "source": [
    "char_batch, char_num_batch = get_char_batch(batch)\n",
    "\n",
    "for win in char_batch:\n",
    "    for w in win:\n",
    "        for c in w:\n",
    "            if c == vocabulary_size:\n",
    "                print(\"0\", end=\" \")\n",
    "                continue\n",
    "            print(id_to_word[c], end=\" \")\n",
    "        print(\" - \", end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "507260"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(id_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed: Tensor(\"embeddings/embedding_lookup:0\", shape=(100, 10, 100), dtype=float32, device=/device:GPU:0)\n",
      "embeddings_concat： Tensor(\"embeddings/concat:0\", shape=(507261, 100), dtype=float32, device=/device:GPU:0)\n",
      "embed_char: Tensor(\"embeddings/embedding_lookup_1:0\", shape=(100, 10, 16, 100), dtype=float32, device=/device:GPU:0)\n",
      "Tensor(\"embedding_lookup:0\", shape=(11, 100), dtype=float32) Tensor(\"truediv:0\", shape=(507260, 100), dtype=float32) Tensor(\"MatMul:0\", shape=(11, 507260), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 200\n",
    "batch_size = 100 # 0509 change\n",
    "# batch_size = 256\n",
    "embedding_size = 100    # Dimension of the embedding vector.\n",
    "skip_window = 5    # How many words to consider left and right.\n",
    "num_skips = 2*skip_window    # How many times to reuse an input to generate a label.\n",
    "num_sampled = 100    # Number of negative examples to sample.\n",
    "# num_sampled = 128    # Number of negative examples to sample.\n",
    "vocabulary_size = len(id_to_word)\n",
    "\n",
    "# We pick a random validation set to sample nearest neighbors. Here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent. These 3 variables are used only for\n",
    "# displaying model accuracy, they don't affect calculation.\n",
    "# valid_size = 16    # Random set of words to evaluate similarity on.\n",
    "# valid_window = 100    # Only pick dev samples in the head of the distribution.\n",
    "# valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "# valid_examples = list(range(1, 10))\n",
    "valid_examples = list(range(280, 291))\n",
    "valid_size = len(valid_examples)\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    with tf.name_scope('inputs'):\n",
    "        train_inputs = tf.placeholder(tf.int32, shape=[batch_size, num_skips])\n",
    "        train_inputs_char = tf.placeholder(tf.int32, shape=[batch_size, num_skips, word_max_len])\n",
    "        train_inputs_char_num = tf.placeholder(tf.float32, shape=[batch_size, num_skips, 1])\n",
    "        train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "        valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "    # Ops and variables pinned to the CPU because of missing GPU implementation\n",
    "    with tf.device('/gpu:0'):\n",
    "#     with tf.device('/gpu:0'):\n",
    "    # Look up embeddings for inputs.\n",
    "        with tf.name_scope('embeddings'):\n",
    "            embeddings = tf.Variable(\n",
    "                    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "            embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "            print(\"embed:\", embed)\n",
    "            \n",
    "            embeddings_concat = tf.concat([embeddings, [[0.0]*embedding_size]], 0)\n",
    "            print(\"embeddings_concat：\", embeddings_concat)\n",
    "            embed_char = tf.nn.embedding_lookup(embeddings_concat, train_inputs_char)\n",
    "            print(\"embed_char:\", embed_char)\n",
    "            embed_mean_char = tf.div(tf.reduce_sum(embed_char, 2), train_inputs_char_num)\n",
    "            \n",
    "            # take mean of embeddings of context words for context embedding\n",
    "            embed_mean = (embed + embed_mean_char) / 2\n",
    "            embed_context = tf.reduce_mean(embed_mean, 1)\n",
    "\n",
    "    with tf.device('/gpu:0'):\n",
    "        # Construct the variables for the NCE loss\n",
    "        with tf.name_scope('weights'):\n",
    "            nce_weights = tf.Variable(\n",
    "                    tf.truncated_normal(\n",
    "                            [vocabulary_size, embedding_size],\n",
    "                            stddev=1.0 / math.sqrt(embedding_size)))\n",
    "        with tf.name_scope('biases'):\n",
    "            nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # Compute the average NCE loss for the batch.\n",
    "    # tf.nce_loss automatically draws a new sample of the negative labels each\n",
    "    # time we evaluate the loss.\n",
    "    # Explanation of the meaning of NCE loss:\n",
    "    #     http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\n",
    "    \n",
    "#     with tf.device('/gpu:2'):\n",
    "    with tf.name_scope('loss'):\n",
    "#         loss = tf.reduce_mean(\n",
    "#             tf.nn.nce_loss(nce_weights, nce_biases, embed_context, train_labels,\n",
    "#                            num_sampled, vocabulary_size))\n",
    "#         print(train_labels, embed_context)\n",
    "        loss = tf.reduce_mean(\n",
    "                tf.nn.nce_loss(\n",
    "                        weights=nce_weights,\n",
    "                        biases=nce_biases,\n",
    "                        labels=train_labels,\n",
    "                        inputs=embed_context,\n",
    "#                         labels=embed_context,\n",
    "#                         inputs=train_labels,\n",
    "                        num_sampled=num_sampled,\n",
    "                        num_classes=vocabulary_size))\n",
    "\n",
    "    # Add the loss value as a scalar to summary.\n",
    "    tf.summary.scalar('loss', loss)\n",
    "\n",
    "    # Construct the SGD optimizer using a learning rate of 1.0.\n",
    "    with tf.name_scope('optimizer'):\n",
    "#         optimizer = tf.train.GradientDescentOptimizer(0.01).minimize(loss)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(1).minimize(loss)\n",
    "#         optimizer = tf.train.AdamOptimizer(0.01).minimize(loss)\n",
    "\n",
    "    # Compute the cosine similarity between minibatch examples and all embeddings.\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    \n",
    "    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "    similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "    print(valid_embeddings, normalized_embeddings, similarity)\n",
    "\n",
    "    # Merge all summaries.\n",
    "    merged = tf.summary.merge_all()\n",
    "\n",
    "    # Add variable initializer.\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Create a saver.\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 20000010\n",
    "# log_dir = \"./log_002_baseline_cbow/\"\n",
    "log_dir = \"./log_005_cwe/\"\n",
    "\n",
    "tfconfig = tf.ConfigProto()\n",
    "tfconfig.gpu_options.allow_growth = True\n",
    "\n",
    "with tf.Session(graph=graph, config=tfconfig) as session:\n",
    "    # Open a writer to write summaries.\n",
    "    writer = tf.summary.FileWriter(log_dir, session.graph)\n",
    "\n",
    "    # We must initialize all variables before we use them.\n",
    "#     init.run()\n",
    "#     saver = tf.train.import_meta_graph('./checkpoint_dir/MyModel-1000.meta')\n",
    "    saver.restore(session, tf.train.latest_checkpoint(log_dir))\n",
    "    print('Initialized')\n",
    "\n",
    "    average_loss = 0\n",
    "    start_index = 880000\n",
    "    for step in xrange(start_index, num_steps):\n",
    "        batch_inputs, batch_labels = generate_batch(batch_size, skip_window=skip_window, num_skips=num_skips)\n",
    "        char_batch, char_num_batch = get_char_batch(batch_inputs)\n",
    "        feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels, \\\n",
    "                     train_inputs_char: char_batch, train_inputs_char_num: char_num_batch}\n",
    "\n",
    "        # Define metadata variable.\n",
    "        run_metadata = tf.RunMetadata()\n",
    "\n",
    "        # We perform one update step by evaluating the optimizer op (including it\n",
    "        # in the list of returned values for session.run()\n",
    "        # Also, evaluate the merged op to get all summaries from the returned \"summary\" variable.\n",
    "        # Feed metadata variable to session for visualizing the graph in TensorBoard.\n",
    "        _, summary, loss_val = session.run(\n",
    "                [optimizer, merged, loss],\n",
    "                feed_dict=feed_dict,\n",
    "                run_metadata=run_metadata)\n",
    "        average_loss += loss_val\n",
    "        \n",
    "        # Add returned summaries to writer in each step.\n",
    "        writer.add_summary(summary, step)\n",
    "        # Add metadata to visualize the graph for the last run.\n",
    "        if step == (num_steps - 1):\n",
    "            writer.add_run_metadata(run_metadata, 'step%d' % step)\n",
    "\n",
    "        if step % 2000 == 0:\n",
    "            if step > 0 and step != start_index:\n",
    "                average_loss /= 2000\n",
    "            # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "            print('Average loss at step ', step, ': ', average_loss)\n",
    "            average_loss = 0\n",
    "\n",
    "        # Note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "        if step % 10000 == 0:\n",
    "            sim = similarity.eval()\n",
    "            for i in xrange(valid_size):\n",
    "                valid_word = id_to_word[valid_examples[i]]\n",
    "                top_k = 8    # number of nearest neighbors\n",
    "                nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "                log_str = 'Nearest to %s:' % valid_word\n",
    "                for k in xrange(top_k):\n",
    "                    close_word = id_to_word[nearest[k]]\n",
    "                    log_str = '%s %s,' % (log_str, close_word)\n",
    "                print(log_str)\n",
    "                \n",
    "            # Save the model for checkpoints.\n",
    "            saver.save(session, os.path.join(log_dir, 'model.ckpt'), global_step=step)\n",
    "        \n",
    "        if step % 1000000 == 0:\n",
    "            word2vec = embeddings.eval()\n",
    "            print(word2vec.shape, type(word2vec))\n",
    "            np.save(\"result/003#cwe_win5_\"+str(step), word2vec)\n",
    "            \n",
    "    final_embeddings = normalized_embeddings.eval()\n",
    "\n",
    "    # Write corresponding labels for the embeddings.\n",
    "    with open(log_dir + '/metadata.tsv', 'w') as f:\n",
    "        for i in xrange(vocabulary_size):\n",
    "            f.write(id_to_word[i] + '\\n')\n",
    "\n",
    "    # Save the model for checkpoints.\n",
    "    saver.save(session, os.path.join(log_dir, 'model.ckpt'), global_step=step)\n",
    "\n",
    "    # Create a configuration for visualizing embeddings with the labels in TensorBoard.\n",
    "    config = projector.ProjectorConfig()\n",
    "    embedding_conf = config.embeddings.add()\n",
    "    embedding_conf.tensor_name = embeddings.name\n",
    "    embedding_conf.metadata_path = os.path.join(log_dir, 'metadata.tsv')\n",
    "    projector.visualize_embeddings(writer, config)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Initialized\n",
    "Average loss at step  880000 :  4.54670953751\n",
    "Nearest to 音乐: 乐, 寄子, 创作, 有着, 王奎林, 加县, 采约, 伊地知,\n",
    "Nearest to 地方: 乡, 州, 庄, 辖, 镇, 各地, 省, 区,\n",
    "Nearest to 社会: 经济, 识, 济, 革, 义, 性, 争, 自由,\n",
    "Nearest to 服务: 假腔, 系统, 弗忌, 供, 店, 裙外, 塍, 前四者,\n",
    "Nearest to 均: 皆, 都, 虽, 仍, 亦, ົ, 留西, 房龙,\n",
    "Nearest to 型: 装, 引擎, WD, WDW, 机, 舰, 猎, 弹,\n",
    "Nearest to 学生: 乳苣, 弗伦克尔, 代站, 少妻, 遭以, 亚维农, 景品, 表妹夫,\n",
    "Nearest to 今: 现, 江, 北, 凶, 镇, 省, 滨, 南,\n",
    "Nearest to 受到: 受, 遭到, 引起, 遭, 造成, 对, 失, 有着,\n",
    "Nearest to 事件: 怖, 战役, 地震, 大规模, 发生, 意外, 科特, 沙布,\n",
    "Nearest to 经济: 社会, 济, 业, 地位, 建设, 促进, 产, 无母数,\n",
    "Average loss at step  882000 :  4.29805609196\n",
    "Average loss at step  884000 :  4.31486256644\n",
    "Average loss at step  886000 :  4.29277227098\n",
    "Average loss at step  888000 :  4.36854749811\n",
    "Average loss at step  890000 :  4.33484858918\n",
    "Nearest to 音乐: 乐, 寄子, 创作, 王奎林, 有着, 加县, 采约, 艺,\n",
    "Nearest to 地方: 庄, 州, 辖, 省, 各地, 区, 民, 三倍,\n",
    "Nearest to 社会: 经济, 识, 济, 革, 义, 性, 争, 自由,\n",
    "Nearest to 服务: 假腔, 系统, 弗忌, 供, 店, 裙外, 塍, 前四者,\n",
    "Nearest to 均: 皆, 都, 虽, 亦, 仍, ົ, 留西, 房龙,\n",
    "Nearest to 型: 装, 引擎, WD, WDW, 猎, 机, 舰, 弹,\n",
    "Nearest to 学生: 乳苣, 弗伦克尔, 代站, 少妻, 遭以, 亚维农, 景品, 武装,\n",
    "Nearest to 今: 现, 江, 北, 镇, 凶, 省, 滨, 南,\n",
    "Nearest to 受到: 受, 遭到, 引起, 遭, 造成, 对, 失, 有着,\n",
    "Nearest to 事件: 怖, 地震, 战役, 大规模, 发生, 意外, 沙布, 科特,\n",
    "Nearest to 经济: 社会, 济, 业, 地位, 建设, 促进, 产, 无母数,\n",
    "Average loss at step  892000 :  4.3156230526\n",
    "Average loss at step  894000 :  4.34211791158\n",
    "Average loss at step  896000 :  4.32540873373\n",
    "Average loss at step  898000 :  4.30250818551\n",
    "Average loss at step  900000 :  4.36458264697\n",
    "Nearest to 音乐: 乐, 寄子, 创作, 王奎林, 有着, 采约, 加县, 深修,\n",
    "Nearest to 地方: 庄, 州, 辖, 各地, 省, 区, 民, 三倍,\n",
    "Nearest to 社会: 经济, 识, 济, 革, 义, 性, 争, 自由,\n",
    "Nearest to 服务: 假腔, 系统, 弗忌, 供, 店, 裙外, 塍, 前四者,\n",
    "Nearest to 均: 皆, 都, 虽, 仍, 亦, ົ, 留西, 加隆,\n",
    "Nearest to 型: 引擎, 装, WDW, WD, 机, 猎, 类, 舰,\n",
    "Nearest to 学生: 乳苣, 弗伦克尔, 代站, 少妻, 遭以, 亚维农, 景品, 亲臣,\n",
    "Nearest to 今: 江, 现, 北, 凶, 镇, 滨, 省, 南,\n",
    "Nearest to 受到: 受, 遭到, 引起, 遭, 造成, 对, 失, 有着,\n",
    "Nearest to 事件: 怖, 地震, 战役, 大规模, 发生, 意外, 沙布, 袭击,\n",
    "Nearest to 经济: 社会, 济, 业, 地位, 建设, 促进, 产, 无母数,\n",
    "Average loss at step  902000 :  4.39618210882\n",
    "Average loss at step  904000 :  4.35122182566\n",
    "Average loss at step  906000 :  4.36412010491\n",
    "Average loss at step  908000 :  4.37509511399\n",
    "Average loss at step  910000 :  4.40667199683\n",
    "Nearest to 音乐: 乐, 寄子, 创作, 有着, 王奎林, 艺, 创, 加县,\n",
    "Nearest to 地方: 庄, 辖, 州, 各地, 省, 乡, 区, 民,\n",
    "Nearest to 社会: 经济, 识, 济, 革, 义, 性, 争, 益,\n",
    "Nearest to 服务: 假腔, 系统, 弗忌, 供, 店, 裙外, 塍, 前四者,\n",
    "Nearest to 均: 皆, 都, 虽, 仍, 亦, ົ, 留西, 加隆,\n",
    "Nearest to 型: 引擎, 装, WDW, WD, 猎, 机, 类, 舰,\n",
    "Nearest to 学生: 乳苣, 弗伦克尔, 代站, 少妻, 遭以, 亚维农, 景品, 表妹夫,\n",
    "Nearest to 今: 江, 北, 现, 镇, 凶, 省, 滨, 南,\n",
    "Nearest to 受到: 受, 遭到, 引起, 遭, 造成, 失, 对, 有着,\n",
    "Nearest to 事件: 怖, 地震, 战役, 大规模, 发生, 意外, 沙布, 科特,\n",
    "Nearest to 经济: 社会, 济, 业, 地位, 建设, 促进, 产, 无母数,\n",
    "Average loss at step  912000 :  4.38963715327\n",
    "Average loss at step  914000 :  4.30611489457\n",
    "Average loss at step  916000 :  4.30013055134\n",
    "Average loss at step  918000 :  4.31839119279\n",
    "Average loss at step  920000 :  4.27445884961\n",
    "Nearest to 音乐: 乐, 寄子, 创作, 有着, 王奎林, 艺, 采约, 加县,\n",
    "Nearest to 地方: 庄, 州, 辖, 各地, 省, 乡, 民, 三倍,\n",
    "Nearest to 社会: 经济, 识, 济, 革, 义, 性, 争, 益,\n",
    "Nearest to 服务: 假腔, 系统, 弗忌, 店, 供, 塍, 裙外, 前四者,\n",
    "Nearest to 均: 皆, 都, 虽, 仍, 亦, ົ, 留西, 加隆,\n",
    "Nearest to 型: 引擎, 装, WD, WDW, 类, 猎, 机, 舰,\n",
    "Nearest to 学生: 乳苣, 弗伦克尔, 代站, 少妻, 遭以, 亚维农, 景品, 表妹夫,\n",
    "Nearest to 今: 江, 现, 北, 镇, 凶, 省, 滨, 南,\n",
    "Nearest to 受到: 受, 遭到, 引起, 遭, 造成, 失, 对, 有着,\n",
    "Nearest to 事件: 怖, 地震, 发生, 战役, 大规模, 意外, 沙布, 袭击,\n",
    "Nearest to 经济: 社会, 济, 业, 地位, 建设, 促进, 产, 无母数,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
